# --- IMPORTS ---
import tensorflow as tf, os, sys
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau
# --- Add src folder to path to import our config ---
sys.path.append(os.path.abspath(os.path.join('..')))
from src import config
# --- DATA GENERATORS ---
train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=30, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, brightness_range=[0.8, 1.2], validation_split=0.2)
validation_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)
train_generator = train_datagen.flow_from_directory(config.DATA_PATH, target_size=config.IMAGE_SIZE, batch_size=config.BATCH_SIZE, class_mode='categorical', subset='training')
validation_generator = validation_datagen.flow_from_directory(config.DATA_PATH, target_size=config.IMAGE_SIZE, batch_size=config.BATCH_SIZE, class_mode='categorical', subset='validation')
num_classes = len(train_generator.class_indices)
# --- PHASE 1: INITIAL TRAINING ---
base_model = MobileNetV2(input_shape=(config.IMAGE_HEIGHT, config.IMAGE_WIDTH, 3), include_top=False, weights='imagenet')
base_model.trainable = False
model = Sequential([base_model, GlobalAveragePooling2D(), Dropout(0.5), Dense(num_classes, activation='softmax')])
model.compile(optimizer=Adam(learning_rate=config.INITIAL_LR), loss='categorical_crossentropy', metrics=['accuracy'])
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1)
print("\n--- Starting INITIAL Training Phase ---")
history = model.fit(train_generator, epochs=config.INITIAL_EPOCHS, validation_data=validation_generator, callbacks=[lr_scheduler])
# --- PHASE 2: FINE-TUNING ---
print("\n--- Starting FINE-TUNING Phase ---")
base_model.trainable = True
for layer in base_model.layers[:config.FINE_TUNE_AT]: layer.trainable = False
model.compile(optimizer=Adam(learning_rate=config.FINE_TUNE_LR), loss='categorical_crossentropy', metrics=['accuracy'])
history_fine_tune = model.fit(train_generator, epochs=config.INITIAL_EPOCHS + config.FINE_TUNE_EPOCHS, initial_epoch=history.epoch[-1], validation_data=validation_generator, callbacks=[lr_scheduler])
# --- SAVE FINAL MODEL ---
model.save(os.path.join(config.MODELS_PATH, config.FINE_TUNED_MODEL_NAME))
print(f"\nâœ… Final optimized model saved!")
